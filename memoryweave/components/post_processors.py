# memoryweave/components/post_processors.py
from typing import Any, Dict, List

from memoryweave.components.base import PostProcessor


class KeywordBoostProcessor(PostProcessor):
    """
    Boosts relevance scores of results containing important keywords.
    """

    def initialize(self, config: dict[str, Any]) -> None:
        """Initialize with configuration."""
        self.keyword_boost_weight = config.get("keyword_boost_weight", 0.5)

    def process_results(
        self, results: list[dict[str, Any]], query: str, context: dict[str, Any]
    ) -> list[dict[str, Any]]:
        """Process retrieved results by boosting for keyword matches."""
        # Get important keywords from query analysis
        keywords = context.get("important_keywords", set())
        if not keywords:
            return results

        # Apply keyword boost
        for result in results:
            content = str(result.get("content", "")).lower()

            # Count keyword matches
            keyword_matches = sum(1 for kw in keywords if kw.lower() in content)

            # Apply boost proportional to matches and weight
            if keyword_matches > 0:
                boost = min(self.keyword_boost_weight * keyword_matches / len(keywords), 0.5)

                # Apply boost to relevance score
                current_score = result.get("relevance_score", 0)
                new_score = min(current_score + boost * (1.0 - current_score), 1.0)
                result["relevance_score"] = new_score
                result["keyword_boost_applied"] = True

        return results


class SemanticCoherenceProcessor(PostProcessor):
    """
    Adjusts relevance scores based on semantic coherence between retrieved results and the query.
    
    This processor analyzes both the query-result coherence and the inter-result coherence,
    ensuring that the set of retrieved memories forms a semantically coherent whole.
    """

    def initialize(self, config: dict[str, Any]) -> None:
        """Initialize with configuration."""
        self.coherence_threshold = config.get("coherence_threshold", 0.2)
        self.enable_query_type_filtering = config.get("enable_query_type_filtering", True)
        self.enable_pairwise_coherence = config.get("enable_pairwise_coherence", True)
        self.enable_clustering = config.get("enable_clustering", False)
        self.min_cluster_size = config.get("min_cluster_size", 2)
        self.max_penalty = config.get("max_penalty", 0.3)
        self.boost_coherent_results = config.get("boost_coherent_results", True)
        self.coherence_boost_factor = config.get("coherence_boost_factor", 0.2)
        self.top_k_outlier_detection = config.get("top_k_outlier_detection", 10)
        
        # Dictionary of query type compatibility
        self.query_type_compatibility = {
            "personal": {"personal": 1.0, "factual": 0.5, "temporal": 0.7, "conceptual": 0.6},
            "factual": {"personal": 0.7, "factual": 1.0, "temporal": 0.8, "conceptual": 0.9},
            "temporal": {"personal": 0.8, "factual": 0.8, "temporal": 1.0, "conceptual": 0.7},
            "conceptual": {"personal": 0.6, "factual": 0.9, "temporal": 0.7, "conceptual": 1.0},
            "default": {"personal": 0.8, "factual": 0.8, "temporal": 0.8, "conceptual": 0.8}
        }

    def process_results(
        self, results: list[dict[str, Any]], query: str, context: dict[str, Any]
    ) -> list[dict[str, Any]]:
        """
        Process retrieved results checking semantic coherence.
        
        This method:
        1. Applies query type filtering to penalize type mismatches
        2. Calculates pairwise coherence between results
        3. Identifies and penalizes outlier results
        4. Boosts coherent result clusters
        5. Updates result scores based on coherence
        
        Args:
            results: List of retrieved results
            query: Original query string
            context: Context dictionary containing query analysis
            
        Returns:
            Updated list of results with adjusted relevance scores
        """
        if len(results) <= 1:
            return results
            
        # Get query type from context
        query_type = context.get("primary_query_type", "default")
        
        # Make a copy of results to modify
        processed_results = list(results)
        
        # 1. Apply query type filtering if enabled
        if self.enable_query_type_filtering:
            processed_results = self._apply_query_type_filtering(processed_results, query_type)
            
        # 2. Calculate pairwise coherence if enabled
        if self.enable_pairwise_coherence and len(processed_results) > 1:
            processed_results = self._apply_pairwise_coherence(processed_results, context)
            
        # 3. Perform clustering and outlier detection if enabled
        if self.enable_clustering and len(processed_results) >= self.min_cluster_size:
            processed_results = self._apply_clustering(processed_results, context)
            
        # 4. Final sort by adjusted relevance score
        processed_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        
        return processed_results
        
    def _apply_query_type_filtering(
        self, 
        results: list[dict[str, Any]], 
        query_type: str
    ) -> list[dict[str, Any]]:
        """Apply penalties for type mismatches between query and results."""
        # Get compatibility matrix for this query type
        compatibility = self.query_type_compatibility.get(query_type, self.query_type_compatibility["default"])
        
        for result in results:
            # Check for type mismatch
            result_type = result.get("type", "unknown")
            
            # If result type is unknown or not in compatibility matrix, use default
            if result_type not in compatibility:
                result_type = "factual"  # Default to factual
                
            # Get compatibility score
            compat_score = compatibility.get(result_type, 0.7)  # Default to 0.7 if not specified
            
            # Apply penalty based on compatibility
            if compat_score < 1.0:
                penalty = (1.0 - compat_score) * self.max_penalty
                result["relevance_score"] = max(0, result.get("relevance_score", 0) - penalty)
                result["type_coherence_applied"] = True
                
        return results
        
    def _apply_pairwise_coherence(
        self, 
        results: list[dict[str, Any]], 
        context: dict[str, Any]
    ) -> list[dict[str, Any]]:
        """Calculate and apply coherence scores between pairs of results."""
        # This requires embeddings to calculate coherence
        embedding_model = context.get("embedding_model")
        
        # Limit pairwise calculation to top-k most relevant results
        top_k = min(self.top_k_outlier_detection, len(results))
        top_results = sorted(results, key=lambda x: x.get("relevance_score", 0), reverse=True)[:top_k]
        
        # If we have embeddings in results, use those
        has_embeddings = all("embedding" in r and r["embedding"] is not None for r in top_results)
        
        if has_embeddings:
            # Get embeddings from results
            embeddings = [r["embedding"] for r in top_results]
            coherence_scores = self._calculate_pairwise_coherence(embeddings)
        elif embedding_model:
            # Generate embeddings from content
            try:
                contents = [str(r.get("content", "")) for r in top_results]
                embeddings = embedding_model.encode(contents)
                coherence_scores = self._calculate_pairwise_coherence(embeddings)
            except Exception:
                # If embedding fails, assign default coherence scores
                coherence_scores = {i: 0.5 for i in range(len(top_results))}
        else:
            # No way to calculate embeddings, assign default coherence scores
            coherence_scores = {i: 0.5 for i in range(len(top_results))}
            
        # Apply coherence scores to results
        for i, result in enumerate(top_results):
            if i in coherence_scores:
                coherence = coherence_scores[i]
                
                # Apply penalty for incoherent results
                if coherence < self.coherence_threshold:
                    penalty = (self.coherence_threshold - coherence) * self.max_penalty
                    result["relevance_score"] = max(0, result.get("relevance_score", 0) - penalty)
                    result["coherence_penalty_applied"] = True
                    
                # Apply boost for highly coherent results
                elif self.boost_coherent_results and coherence > (1.0 - self.coherence_threshold):
                    boost = coherence * self.coherence_boost_factor
                    current_score = result.get("relevance_score", 0)
                    result["relevance_score"] = min(1.0, current_score + boost * (1.0 - current_score))
                    result["coherence_boost_applied"] = True
                    
                # Store coherence score
                result["coherence_score"] = coherence
                
        # Update the original results list
        result_ids = {id(r) for r in top_results}
        for i, result in enumerate(results):
            if id(result) in result_ids:
                # This result was processed, find its updated version
                for updated in top_results:
                    if id(updated) == id(result):
                        results[i] = updated
                        break
                        
        return results
        
    def _calculate_pairwise_coherence(self, embeddings) -> dict[int, float]:
        """
        Calculate coherence scores for each result based on similarity to other results.
        
        Args:
            embeddings: List of embedding vectors
            
        Returns:
            Dictionary mapping result index to coherence score
        """
        import numpy as np
        
        if len(embeddings) <= 1:
            return {0: 1.0} if embeddings else {}
            
        # Convert to numpy array if not already
        if not isinstance(embeddings, np.ndarray):
            embeddings = np.array(embeddings)
            
        # Normalize embeddings
        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
        norms[norms == 0] = 1e-10  # Avoid division by zero
        normalized_embeddings = embeddings / norms
        
        # Calculate pairwise similarities
        similarities = np.dot(normalized_embeddings, normalized_embeddings.T)
        
        # Exclude self-similarity
        np.fill_diagonal(similarities, 0)
        
        # Calculate average similarity for each result (coherence score)
        coherence_scores = {}
        for i in range(len(embeddings)):
            # Average similarity to all other results
            coherence_scores[i] = float(np.sum(similarities[i]) / (len(embeddings) - 1))
            
        return coherence_scores
        
    def _apply_clustering(
        self, 
        results: list[dict[str, Any]], 
        context: dict[str, Any]
    ) -> list[dict[str, Any]]:
        """
        Apply clustering to identify coherent groups and outliers.
        
        This is a more sophisticated approach that uses clustering
        algorithms to identify coherent groups of results.
        """
        try:
            # This requires embeddings and sklearn
            import numpy as np
            from sklearn.cluster import DBSCAN
            
            # Get embeddings
            embedding_model = context.get("embedding_model")
            
            if embedding_model:
                # Generate embeddings from content
                contents = [str(r.get("content", "")) for r in results]
                embeddings = embedding_model.encode(contents)
            else:
                # Try to get embeddings from results
                embeddings = []
                for r in results:
                    if "embedding" in r and r["embedding"] is not None:
                        embeddings.append(r["embedding"])
                    else:
                        # If any result is missing embedding, can't proceed
                        return results
                        
            # Convert to numpy array
            embeddings = np.array(embeddings)
            
            # Normalize embeddings
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
            norms[norms == 0] = 1e-10  # Avoid division by zero
            normalized_embeddings = embeddings / norms
            
            # Calculate pairwise distances (cosine distance = 1 - cosine similarity)
            similarities = np.dot(normalized_embeddings, normalized_embeddings.T)
            distances = 1 - similarities
            
            # Perform DBSCAN clustering
            eps = 1 - self.coherence_threshold  # Convert coherence threshold to distance
            min_samples = min(self.min_cluster_size, len(results) // 2)
            db = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed').fit(distances)
            
            # Get cluster labels (-1 indicates outliers)
            labels = db.labels_
            
            # Count clusters
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            
            # Apply adjustments based on clustering
            for i, result in enumerate(results):
                if i < len(labels):
                    cluster_id = labels[i]
                    result["cluster_id"] = int(cluster_id)
                    
                    if cluster_id == -1:
                        # Outlier - apply penalty
                        penalty = self.max_penalty
                        result["relevance_score"] = max(0, result.get("relevance_score", 0) - penalty)
                        result["outlier_penalty_applied"] = True
                    else:
                        # In a cluster - count cluster size
                        cluster_size = np.sum(labels == cluster_id)
                        
                        # Boost based on cluster size - larger clusters are more coherent
                        if self.boost_coherent_results:
                            size_factor = min(cluster_size / len(results), 0.8)
                            boost = size_factor * self.coherence_boost_factor
                            current_score = result.get("relevance_score", 0)
                            result["relevance_score"] = min(1.0, current_score + boost * (1.0 - current_score))
                            result["cluster_boost_applied"] = True
                            result["cluster_size"] = int(cluster_size)
                
        except (ImportError, Exception) as e:
            # If clustering fails, fall back to pairwise coherence
            import logging
            logging.warning(f"Clustering failed with error: {str(e)}. Falling back to pairwise coherence.")
            if not any("coherence_score" in r for r in results):
                results = self._apply_pairwise_coherence(results, context)
                
        return results


class AdaptiveKProcessor(PostProcessor):
    """
    Adjusts the number of results based on query characteristics.
    """

    def initialize(self, config: dict[str, Any]) -> None:
        """Initialize with configuration."""
        self.adaptive_k_factor = config.get("adaptive_k_factor", 0.3)

    def process_results(
        self, results: list[dict[str, Any]], query: str, context: dict[str, Any]
    ) -> list[dict[str, Any]]:
        """Process retrieved results by adjusting the number based on scores."""
        if not results:
            return results

        # Get original top_k
        original_k = context.get("top_k", 5)

        # Check result quality
        avg_score = sum(r.get("relevance_score", 0) for r in results) / len(results)

        # Adjust number of results based on quality
        if avg_score > 0.7:
            # High quality results - keep fewer
            adaptive_k = max(1, int(original_k * (1.0 - self.adaptive_k_factor)))
            return sorted(results, key=lambda x: x.get("relevance_score", 0), reverse=True)[
                :adaptive_k
            ]
        elif avg_score < 0.3:
            # Low quality results - keep more for diversity
            return results
        else:
            # Medium quality - sort and return original amount
            return sorted(results, key=lambda x: x.get("relevance_score", 0), reverse=True)[
                :original_k
            ]


class MinimumResultGuaranteeProcessor(PostProcessor):
    """
    Ensures a minimum number of results are returned even if they don't meet the confidence threshold.
    
    This processor implements a fallback retrieval strategy when the initial retrieval
    doesn't return enough results, ensuring that queries always receive a response.
    """
    
    def initialize(self, config: dict[str, Any]) -> None:
        """Initialize with configuration."""
        self.min_results = config.get("min_results", 1)
        self.fallback_threshold_factor = config.get("fallback_threshold_factor", 0.5)
        self.min_fallback_threshold = config.get("min_fallback_threshold", 0.05)
        self.memory = config.get("memory", None)
        
    def process_results(
        self, results: list[dict[str, Any]], query: str, context: dict[str, Any]
    ) -> list[dict[str, Any]]:
        """Process retrieved results by ensuring a minimum number of results."""
        # If we already have enough results, no action needed
        if len(results) >= self.min_results:
            return results
            
        # Check if we have access to the necessary components to perform fallback retrieval
        if not self.memory or "query_embedding" not in context:
            return results
            
        # Get the original confidence threshold used
        original_threshold = context.get("confidence_threshold", 0.0)
        
        # Calculate fallback threshold - lower than original but with a minimum
        fallback_threshold = max(
            self.min_fallback_threshold, 
            original_threshold * self.fallback_threshold_factor
        )
        
        # Get existing memory IDs to avoid duplicates
        existing_ids = {r.get("id") for r in results if "id" in r}
        
        # Try to get additional results with lower threshold
        try:
            # If memory has a direct similarity search method
            if hasattr(self.memory, "search_by_embedding"):
                # Calculate how many more results we need
                additional_count = self.min_results - len(results)
                
                # Get query embedding
                query_embedding = context["query_embedding"]
                
                # Get additional results with lower threshold
                fallback_results = self.memory.search_by_embedding(
                    query_embedding, 
                    k=additional_count + len(existing_ids),  # Request extra to account for duplicates
                    threshold=fallback_threshold
                )
                
                # Filter out existing IDs
                fallback_results = [
                    r for r in fallback_results if r.get("id") not in existing_ids
                ]
                
                # Add to the original results until min_results is reached
                for result in fallback_results[:additional_count]:
                    result["from_fallback"] = True
                    results.append(result)
                    
        except Exception as e:
            # Log error but don't fail
            print(f"Error in minimum result guarantee fallback: {str(e)}")
            
        return results


class PersonalAttributeProcessor(PostProcessor):
    """
    Enhances retrieval results based on personal attributes from the query.
    
    This processor analyzes the query for personal attribute references and boosts
    results that contain relevant attributes. It can also generate synthetic
    attribute memory entries for direct attribute questions.
    """
    
    def initialize(self, config: dict[str, Any]) -> None:
        """Initialize with configuration."""
        self.attribute_boost_factor = config.get("attribute_boost_factor", 0.6)
        self.add_direct_responses = config.get("add_direct_responses", True)
        self.min_relevance_threshold = config.get("min_relevance_threshold", 0.3)
    
    def process_results(
        self, results: list[dict[str, Any]], query: str, context: dict[str, Any]
    ) -> list[dict[str, Any]]:
        """Process retrieved results by incorporating personal attributes."""
        # Check if personal attributes are available in context
        if "personal_attributes" not in context:
            return results
            
        personal_attributes = context.get("personal_attributes", {})
        relevant_attributes = context.get("relevant_attributes", {})
        
        if not relevant_attributes:
            return results
            
        # Create a copy of results to modify
        enhanced_results = list(results)
        
        # 1. Boost existing results that contain relevant attributes
        for result in enhanced_results:
            content = str(result.get("content", "")).lower()
            
            # Check for attribute matches in content
            attribute_matches = 0
            for attr_type, attr_value in relevant_attributes.items():
                if isinstance(attr_value, str) and attr_value.lower() in content:
                    attribute_matches += 1
                elif isinstance(attr_value, list):
                    for value in attr_value:
                        if value.lower() in content:
                            attribute_matches += 1
            
            # Apply boost based on matches
            if attribute_matches > 0:
                boost = min(self.attribute_boost_factor * attribute_matches / len(relevant_attributes), 0.8)
                current_score = result.get("relevance_score", 0)
                new_score = min(current_score + boost * (1.0 - current_score), 1.0)
                result["relevance_score"] = new_score
                result["attribute_boost_applied"] = True
        
        # 2. For direct attribute questions, create a synthetic result if needed
        if self.add_direct_responses and relevant_attributes:
            # Check if query is likely a direct question about an attribute
            direct_query_types = ["what is my", "where do i", "who is my", "tell me my", "what's my"]
            is_direct_query = any(query.lower().startswith(prefix) for prefix in direct_query_types)
            
            # If direct query and no high relevance results exist, create synthetic response
            has_high_relevance = any(r.get("relevance_score", 0) > self.min_relevance_threshold for r in enhanced_results)
            
            if is_direct_query and (not has_high_relevance or not enhanced_results):
                # Create synthetic attribute response
                attribute_memory = self._create_attribute_memory(query, relevant_attributes)
                if attribute_memory:
                    # Add as highest relevance result
                    enhanced_results.insert(0, attribute_memory)
        
        return enhanced_results
    
    def _create_attribute_memory(self, query: str, relevant_attributes: dict[str, Any]) -> dict[str, Any]:
        """Create a synthetic memory entry from personal attributes relevant to the query."""
        if not relevant_attributes:
            return None
            
        # Determine which attribute is most relevant to query
        attr_key = next(iter(relevant_attributes.keys()))
        attr_value = relevant_attributes[attr_key]
        
        # Format depends on attribute type
        attr_category = attr_key.split("_")[0] if "_" in attr_key else "attribute"
        attr_type = attr_key.split("_")[1] if "_" in attr_key else attr_key
        
        # Generate content based on attribute type
        if attr_category == "preferences":
            content = f"Your favorite {attr_type} is {attr_value}."
        elif attr_category == "demographic":
            if attr_type == "location":
                content = f"You live in {attr_value}."
            elif attr_type == "occupation":
                content = f"You work as a {attr_value}."
            else:
                content = f"Your {attr_type} is {attr_value}."
        elif attr_category == "relationship":
            content = f"Your {attr_type} is {attr_value}."
        elif attr_category == "trait":
            if attr_type == "hobbies" and isinstance(attr_value, list):
                hobbies_str = ", ".join(attr_value)
                content = f"Your hobbies include {hobbies_str}."
            else:
                content = f"Your {attr_type} is {attr_value}."
        else:
            content = f"Your {attr_type} is {attr_value}."
        
        # Create memory entry
        return {
            "content": content,
            "relevance_score": 1.0,  # Highest relevance
            "type": "attribute",
            "source": "personal_attribute",
            "embedding": None,
            "id": f"attribute-{attr_key}",
            "timestamp": None,
            "is_synthetic": True
        }
